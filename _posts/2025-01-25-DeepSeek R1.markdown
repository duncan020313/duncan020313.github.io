---
layout: post
title:  "About DeepSeek R1"
date:   2025-01-25
categories: AI, Reasoning Model, Reinforcement Learning
---
## 배울 점
- 모델에게 직접적인 추론 방법을 가르치는 것보다 적절히 Incentive를 주는 것이 훨씬 효과적임
- 정답 채점기만 제공하고, 문제 푸는 방법은 스스로 학습하도록 하는게 낫다는 것
	- 알파고가 그랬던 것처럼 인간의 방식을 가르치는 것보다 스스로 학습하게 만드는 게 낫다
- 소형 모델은 RL로 학습하는 것보다 대형 모델이 추론한 방식을 그대로 따라하도록 Distillation 하는게 낫다

## 아이디어 요약
- 추론 모델의 비법은 순수 RL로 모델에게 추론 방식을 스스로 찾아내게 만드는 것
  - 그런데, 순수 RL만 쓰면 모델 추론 과정의 가독성이 너무 떨어지고 여러 언어를 섞어서 사용하는 문제가 발생함
- 위 문제를 해결하기 위해 4단계에 걸쳐서 모델을 훈련함
  - SFT (Cold Start) -> RL -> SFT (Reject Sampling) -> RL
- RL 훈련에는 GRPO 사용
  - GRPO는 쉽게 말하면 한 번의 정책 (policy) 업데이트에 여러가지 출력을 샘플링하고, reward가 높은 Output이 나올 확률을 높이고, reward가 상대적으로 낮은 Output이 나올 확률을 낮추도록 학습
  - 여러가지 출력의 평균치로 정책을 업데이트 하므로 정책이 안정적으로 학습됨
- 보상 modeling에는 규칙 기반 보상 사용
  - Accuracy reward: 정답을 맞추면 리워드 제공 (수학 문제는 문제 답을 맞추면 리워드 제공, 코딩 문제는 테스트케이스 통과하면 리워드 제공)
  - Format reward: 모델이 \<think\>\</think\> 사이에서 Reasoning을 수행하면 리워드 제공
  - 여기서 Process Reward Model 같은건 사용되지 않음
    - 큰 모델에 RL을 사용하면 이런 PRM이 해킹 (편법으로 좋은 결과를 내도록 모델이 학습됨. 리워드 모델에 오버피팅)되기 쉬움

## 훈련 결과
#### DeepSeek-R1 Zero (순수 RL 모델)의 성능 변화
- 훈련 스텝이 진행됨에 따라 지속적으로 AIME 정확도가 상승했으며, 최종적으로는 o1 모델을 이김
- 재밌는 점은 어떤 Supervized 데이터가 사용되지 않았다는 점
  - 마치 알파고보다 알파고 제로가 더 뛰어난 성능을 보이는 것과 비슷함
- 모델은 훈련 시간이 지남에 따라 성능 향상도 일어났지만 그와 동시에 추론 길이도 함께 늘어남
  - 결국 긴 추론 길이가 정확도 향상과 강한 상관관계를 가진다는 것을 알 수 있음
  - RL 훈련 공식에는 추론 길이에 대한 어떠한 incentive도 없었지만, 모델이 자체적으로 이러한 방향으로 발전함
  - 또한, 훈련을 거듭할수록 모델이 자신의 출력을 점검하는 자체 점검 능력이 자동적으로 출현하게 됨
  - 모델은 추론 중간에 Aha moment를 깨우치고 기존 접근 방식을 재평가하여 문제에 더 많은 사고 시간을 할당하는 방법을 학습
- 문제점
  - 가독성 떨어짐
  - 여러 언어 섞어서 말함 (Language Mixing 현상)


#### DeepSeek-R1: RL with Cold Start
- 2가지 질문
1. Q1: RL의 수렴 속도를 고품질 데이터로 Cold Start하여 가속할 수 있을까?
2. Q2: 모델이 유저 친화적인 CoT 출력을 생성하면서 동시에 강한 추론 능력을 가지도록 만들 수 있을까?

- 이 두 가지 질문에 답하기 위해서 모델을 4단계로 훈련
1. Cold Start
   - 초기 불안정한 RL 훈련 시기를 빠르게 넘기기 위해서 소규모의 long CoT 데이터를 구축하여 SFT 수행
   - 이 데이터는 여러가지 방법 (Zero 모델에서 수집, 사람이 이를 적절히 포맷팅하기 등등)으로 수집됨
2. Reasoning-Oriented RL
   - DeepSeek-Zero와 같은 방식으로 훈련함
3. Reject Sampling + SFT
   - RL 훈련이 어느 정도 수렴하면, 해당 모델 체크포인트에서 SFT 훈련용 데이터를 수집함
   - Cold-start 데이터와는 달리 추론 관련 문제들 뿐만 아니라 다양한 도메인에서 문제들을 (writing, role-playing 등 General purpose 문제들) 구성하여 모델에서 합성 데이터를 뽑음
     - 추론 문제들의 경우 Reject Sampling을 통해 모델이 정답을 맞힌 경우들만 걸러서 데이터 수집. 이전과 차이점은 이전에는 규칙 기반으로 정확히 판단할 수 있는 경우만 모았지만, 이번에는 DeepSeek-V3를 Judgement로 사용하여 좀 더 다양한 데이터를 대상으로 수집
     - mixed langauge 현상이 발생했거나, 지나치게 긴 paragraph, code block이 사용된 경우 거부
     - 비추론 문제들의 경우 DeepSeek-V3를 파이프라인에 넣거나 DeepSeek-V3 훈련에 사용했던 SFT 데이터를 사용함
4. RL for all scenarios
   - 모델을 인간의 선호도에 더욱 맞추기 위해 모델의 유용성과 무해성을 개선하는 동시에 추론 능력을 개선하기 위한 2차 강화 학습 수행
   - 추론 문제는 기존 방법 동일하게 사용
   - 비추론 문제는 인간 선호도 판별을 위해 보상 모델 사용


#### Distillation
- DeepSeek-R1으로부터 합성된 800만개 샘플을 사용하여 다른 모델에 Fine-tuning 수행
- RL 안쓰고 이 간단한 방법만으로 성능 크게 향상됨


## 실패 사례들

#### PRM 기반 훈련
- 일반적인 추론에서 세분화된 단계를 명시적으로 정의하는 것이 어려움
- 현재 중간 단계가 올바른지 판단하는 것이 어려움
- 모델을 사용한 자동화된 주석은 만족스러운 결과를 얻지 못할 수 있으며, 수동 주석은 확장성이 떨어짐
- 모델 기반 PRM을 도입하면 필연적으로 리워드 해킹이 발생. 보상 모델을 재학습하려면 추가 학습 리소스가 필요하며 전체 학습 파이프라인이 복잡해짐

#### MCTS 기반 추론
- 탐색 공간이 비교적 잘 정의된 체스와 달리 토큰 생성은 기하급수적으로 더 큰 탐색 공간을 가짐
- 값 모델은 탐색 과정의 각 단계를 안내하기 때문에 생성 품질에 직접적인 영향을 미치고, 세분화된 값 모델을 훈련하는 것은 본질적으로 어렵기 때문에 모델을 반복적으로 개선하기가 어려움
- 결국 값 모델이 존재하면 MCTS로 추론할 수 있지만, 값 모델을 훈련하는 것 자체가 어려움. 결국 모델이 스스로 자체 검색을 통해서 최적의 추론 경로를 찾는 것이 중요
